general_settings:
  master_key: ""
  # Optional: Add authentication if needed
  # master_key: "your-secret-key"

model_list:
  # Primary local coding model - optimized for code completion
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: "ollama_chat/qwen2.5-coder:7b"
      api_base: "http://host.docker.internal:11434"
      keep_alive: "5m"
      temperature: 0.1
      max_tokens: 2048

  # Alternative coding model for fallback
  - model_name: gpt-4
    litellm_params:
      model: "ollama_chat/codellama:13b"
      api_base: "http://host.docker.internal:11434"
      keep_alive: "5m"
      temperature: 0.1
      max_tokens: 2048

  # General purpose model for chat/explanations
  - model_name: gpt-4o
    litellm_params:
      model: "ollama_chat/llama3.2:8b"
      api_base: "http://host.docker.internal:11434"
      keep_alive: "5m"
      temperature: 0.3
      max_tokens: 4096

router_settings:
  fallbacks:
    - { "gpt-3.5-turbo": ["gpt-4", "gpt-4o"] }
    - { "gpt-4": ["gpt-3.5-turbo", "gpt-4o"] }
    - { "gpt-4o": ["gpt-4", "gpt-3.5-turbo"] }
  
  # Load balancing settings
  routing_strategy: "least-busy"
  
  # Retry settings
  num_retries: 2
  timeout: 30
