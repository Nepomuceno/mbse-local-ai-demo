#!/bin/bash
# setup_local_ai.sh - Automated setup script for Local AI with GitHub Copilot

set -e

echo "ğŸš€ Setting up Local AI for GitHub Copilot..."

# Check if Docker is running
if ! docker info > /dev/null 2>&1; then
    echo "âŒ Docker is not running. Please start Docker first."
    exit 1
fi

# Check if Ollama is installed
if ! command -v ollama &> /dev/null; then
    echo "ğŸ“¦ Installing Ollama..."
    curl -fsSL https://ollama.com/install.sh | sh
else
    echo "âœ… Ollama is already installed"
fi

# Start Ollama if not running
if ! pgrep -f "ollama serve" > /dev/null; then
    echo "ğŸ”„ Starting Ollama..."
    ollama serve &
    sleep 5
fi

# Download recommended models
echo "ğŸ“¥ Downloading recommended AI models..."
echo "This may take a while depending on your internet connection..."

models_to_download=(
    "qwen2.5-coder:7b"
    "codellama:13b"
    "llama3.2:8b"
)

for model in "${models_to_download[@]}"; do
    echo "ğŸ“¥ Downloading $model..."
    ollama pull "$model"
done

# Check if hosts file needs modification
if ! grep -q "127.0.0.1 api.openai.com" /etc/hosts; then
    echo "ğŸ”§ Modifying hosts file..."
    echo "127.0.0.1 api.openai.com" | sudo tee -a /etc/hosts
    echo "âœ… Hosts file updated"
else
    echo "âœ… Hosts file already configured"
fi

# Start LiteLLM service
echo "ğŸ”„ Starting LiteLLM service..."
docker-compose up -d

# Wait for services to be ready
echo "â³ Waiting for services to start..."
sleep 10

# Test the setup
echo "ğŸ§ª Testing the setup..."

# Test Ollama
echo "Testing Ollama..."
if curl -s http://localhost:11434/api/tags > /dev/null; then
    echo "âœ… Ollama is running"
else
    echo "âŒ Ollama is not responding"
    exit 1
fi

# Test LiteLLM
echo "Testing LiteLLM..."
if curl -s http://localhost:4000/health > /dev/null; then
    echo "âœ… LiteLLM is running"
else
    echo "âŒ LiteLLM is not responding"
    exit 1
fi

# Test with a simple request
echo "Testing AI inference..."
response=$(curl -s -X POST http://localhost:4000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer test-key" \
    -d '{
        "model": "gpt-3.5-turbo",
        "messages": [{"role": "user", "content": "Hello, world!"}],
        "max_tokens": 50
    }')

if echo "$response" | grep -q "choices"; then
    echo "âœ… AI inference is working"
else
    echo "âŒ AI inference test failed"
    echo "Response: $response"
fi

echo ""
echo "ğŸ‰ Setup complete! Your local AI is now ready for GitHub Copilot."
echo ""
echo "ğŸ“‹ Next steps:"
echo "1. Start Caddy with: sudo caddy run --config Caddyfile"
echo "2. Open VS Code and use GitHub Copilot normally"
echo "3. Your requests will now be handled by your local AI!"
echo ""
echo "ğŸ”§ To customize your models, edit litellm_config.yaml"
echo "ğŸ“š See README.md for detailed configuration options"
echo ""
echo "ğŸ’¡ Tip: Monitor your setup with:"
echo "   - Ollama: curl http://localhost:11434/api/tags"
echo "   - LiteLLM: curl http://localhost:4000/health"
echo "   - Logs: docker logs litellm"
