# ğŸš€ MBSE Local AI

A demonstration of local AI integration with GitHub Copilot and specialized document processing using MCP servers.

## Overview

This project demonstrates the integration of local AI models with development tools and specialized document processing capabilities. It provides a practical example of how to enhance developer workflows while maintaining data privacy and offline functionality.

The project includes two main components:

1. **ğŸ”„ Local AI Integration**: A local AI system that intercepts GitHub Copilot requests and routes them to local models (Ollama) with Azure OpenAI fallback, providing enhanced privacy and offline capabilities.

2. **ğŸ§  MCP Server**: A Model Context Protocol server that processes PYRAMID specification documents and provides specialized knowledge for systems engineering requirements analysis.

## âœ¨ Features

- **ğŸ  Local AI Integration**: Complete setup for routing GitHub Copilot requests to local models
- **ğŸ¤– MCP Server**: Model Context Protocol server for specialized document processing
- **ğŸ“‹ PYRAMID Specification Support**: Process and analyze PYRAMID technical standard documents
- **ğŸŒ Offline Capabilities**: Runs completely offline while enhancing developer capabilities
- **ğŸš€ Development Container**: Pre-configured development environment with VS Code integration
- **ğŸ”§ Extensible Architecture**: Designed to be extended with additional AI models and document types

## ğŸ› ï¸ Requirements

- **Git LFS**: Required for handling large PYRAMID specification documents
- Python 3.12+
- Docker and Docker Compose
- MCP (Model Context Protocol) Python SDK
- UV (for dependency management)
- VS Code with dev container support

### ğŸª Git LFS Setup

This project uses Git LFS (Large File Storage) to handle large PDF documents in the `mcp_server/data/` directory.

#### First-time Git LFS Setup

```bash
# Install Git LFS (if not already installed)
# On Ubuntu/Debian:
sudo apt-get install git-lfs

# On macOS:
brew install git-lfs

# On Windows:
# Download from: https://git-lfs.github.io/

# Initialize Git LFS for your user account
git lfs install
```

#### Working with this Repository

```bash
# Clone the repository (Git LFS files will be downloaded automatically)
git clone <repository-url>
cd mbse-local-ai

# If you cloned before LFS setup, pull LFS files:
git lfs pull

# Check LFS status
git lfs ls-files
```

**Note**: The dev container automatically configures Git LFS.

## ğŸš€ Installation

### Method 1: Quick Start with Dev Container (Recommended)

1. **Prerequisites**: Ensure Git LFS is installed and configured (see requirements above)

2. **Clone Repository**: 
   ```bash
   git clone <repository-url>
   cd mbse-local-ai
   ```

3. **Open in Dev Container**: Open the project in VS Code and select "Reopen in Container" when prompted. The dev container will automatically:
   - Install Git LFS and configure it
   - Install all dependencies
   - Configure the MCP server
   - Set up the complete development environment

4. **Setup Local AI (Optional)**: Follow the guide in `local-config/README.md` to set up the local AI system.

### Method 2: Manual Installation

```bash
# Ensure Git LFS is installed and configured
git lfs install

# Clone the repository
git clone <repository-url>
cd mbse-local-ai

# Pull LFS files if needed
git lfs pull

# Create virtual environment and install dependencies
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install -e .
```

## ğŸ® Usage

### ğŸ”§ Local AI Setup

Configure GitHub Copilot to use local AI models:

1. **Follow Setup Guide**: See `local-config/README.md` for complete instructions
2. **Start Local Services**: Run `docker-compose up -d` in the `local-config` directory
3. **Configure Hosts**: Modify your hosts file to redirect OpenAI API calls to local setup

### ğŸ§  MCP Server for Document Processing

Run the MCP server to process PYRAMID specification documents:

```bash
# Start the MCP server
python mcp_server/server.py
```

The MCP server provides tools for:
- **ğŸ“‹ Document Analysis**: Process PYRAMID technical standard documents
- **âœ… Compliance Tools**: Check requirements against PYRAMID specifications
- **ğŸ” PRA Analysis**: Analyze component impact for requirements changes
- **ğŸ“ File Processing**: Handle large PDF documents with metadata extraction

### ğŸª Demo Examples

The `demo/` folder contains a sample requirement diagram in Mermaid format. Try asking the MCP server to:

- ğŸŒ¡ï¸ "Add temperature monitoring and alerts to the fuel reading requirement"
- ğŸ” "Identify which components would be impacted by adding temperature sensors"
- ğŸ“‹ "Modify the requirements to include engine temperature limits while maintaining the Mermaid diagram format"
- ğŸ“Š "Perform a PRA analysis for the temperature alert requirement changes"

## ğŸ¤– MCP Server Integration

The MCP server provides specialized knowledge processing for PYRAMID specifications and systems engineering documents. It runs completely offline while enhancing developer capabilities.

### ğŸ¯ MCP Server Features

- **ğŸ“„ Document Processing**: Extract and analyze PYRAMID technical standard documents
- **âœ… Compliance Analysis**: Check requirements against PYRAMID specifications
- **ğŸ“Š PRA Tools**: Perform Probabilistic Risk Assessment for component changes
- **ğŸ“‹ PDF Processing**: Handle large PDF documents with metadata extraction
- **ğŸ”§ Requirements Modification**: Modify system requirements while maintaining traceability

### ğŸš€ Using MCP Server in VS Code

1. **âš¡ Auto-Configuration**: The MCP server is pre-configured in the dev container
2. **ğŸ¤– Agent Mode**: Open Chat (âŒƒâŒ˜I), select "Agent" mode, and click "Tools"
3. **ğŸ¯ Specialized Queries**: Ask questions about PYRAMID specifications or requirements analysis

### ğŸª Example MCP Queries

- ğŸ“‹ "List all PYRAMID documents in the data directory"
- ğŸ” "Analyze the fuel monitoring requirements and suggest temperature integration"
- ğŸ¤” "What components would be affected by adding temperature sensors to the fuel system?"
- ğŸ“Š "Modify the requirements to include engine temperature monitoring while preserving the Mermaid diagram structure"

## ğŸ› ï¸ Development

### ğŸš€ Development Container

Run the project from the supplied dev container:

- **ğŸŒŸ Complete Environment**: Python 3.12, UV, Docker, Git LFS, and all dependencies pre-installed
- **ğŸ”§ VS Code Extensions**: Pre-configured with Python, Mermaid, Copilot, and MCP support
- **âš¡ Auto-Setup**: Automatic dependency installation, Git LFS configuration, and MCP server setup
- **ğŸŒ Offline Capable**: Full functionality without internet access once configured
- **ğŸ“‹ LFS Support**: Automatic Git LFS initialization and configuration for large documents

### ğŸ—ï¸ Project Structure

```
mbse-local-ai/
â”œâ”€â”€ .devcontainer/            # Complete dev container setup
â”œâ”€â”€ .vscode/                  # VS Code configuration with MCP enabled
â”œâ”€â”€ mcp_server/               # MCP server for document processing
â”‚   â”œâ”€â”€ server.py            # Main MCP server implementation
â”‚   â”œâ”€â”€ document_parser.py   # Document processing utilities
â”‚   â”œâ”€â”€ pdf_processor.py     # PDF handling and metadata extraction
â”‚   â”œâ”€â”€ data/                # PYRAMID specification documents
â”‚   â””â”€â”€ tools/               # Specialized tools (compliance, PRA, documents)
â”œâ”€â”€ demo/                     # Sample requirements in Mermaid format
â”‚   â””â”€â”€ requirement.mmd      # Example fuel monitoring system requirements
â”œâ”€â”€ local-config/            # Local AI setup for GitHub Copilot
â”‚   â”œâ”€â”€ docker-compose.yaml # LiteLLM and Caddy proxy setup
â”‚   â”œâ”€â”€ Caddyfile           # Reverse proxy configuration
â”‚   â””â”€â”€ README.md           # Complete setup instructions
â””â”€â”€ pyproject.toml          # Project configuration
```

## âš™ï¸ Configuration

### ğŸš€ Local AI Configuration

The `local-config/` directory contains complete setup for GitHub Copilot local AI integration:

- **ğŸ³ Docker Compose**: LiteLLM server with Ollama integration and Azure OpenAI fallback
- **ğŸ”„ Caddy Proxy**: SSL-enabled reverse proxy for intercepting OpenAI API calls
- **ğŸ”’ SSL Certificates**: Pre-configured certificates for `api.openai.com` interception
- **âš™ï¸ Configuration Files**: Multiple LiteLLM configurations for different use cases

### ğŸ¤– MCP Server Configuration

The MCP server is automatically configured in `.vscode/mcp.json` for seamless VS Code integration with specialized tools for PYRAMID document processing and requirements analysis.

---

## Summary

This project provides a practical example of integrating local AI models with development workflows while maintaining privacy and offline capabilities. The combination of local AI processing, specialized document handling, and developer-friendly tools demonstrates how to build enhanced development environments.

The project is designed to be extended and modified for different use cases and document types.

---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
